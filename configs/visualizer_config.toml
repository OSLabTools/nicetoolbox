[io]
dataset_folder = "<datasets_folder_path>"                                 # main dataset folder
nice_tool_input_folder = "<output_folder_path>/nicetoolbox_input/<dataset_name>_<session_ID>_<sequence_ID>" # raw_processed input data
nice_tool_output_folder = "<output_folder_path>/experiments"              # NICE Toolbox experiment output
experiment_folder = "<output_folder_path>/experiments/<yyyymmdd>"         # select single NICE Toolbox experiment output folder
experiment_video_folder = "<experiment_folder>/<video_name>"              # NICE Toolbox output folder for the specific video
experiment_video_component = "<experiment_video_folder>/<component_name>" # NICE Toolbox output folder for the specific component

[media]                                                    # each Media session shows one video results.
dataset_name = 'communication_multiview'                   # dataset of the video
video_name = 'communication_multiview_session_xyz_s0_l99'  # name of video result folder
multi_view = true                                          # true if you have multiple cameras, otherwise set it to false
[media.visualize]                                          # specify what will be visualized
components = ["body_joints", "hand_joints", "face_landmarks", "head_orientation", "gaze_individual", "gaze_interaction", "emotion_individual", "kinematics", "proximity"]
camera_position = true                                     # true if you want to visuailize camera position -- requires extrinsic information of the camera
start_frame = 0                                            # starting frame for the visualization
end_frame = -1                                             # end frame for the visualization, -1 means process until the end of the video
visualize_interval = 1                                     # 1 means visualize every frame; change the parameter accordingly if you want to visualize every x frames

###### Configuring Component Data Display in Rerun Windows #####
### You can control which data will be shown in specific rerun windows by adjusting the 'media.component.canvas' items
### The keys (like '3d' or '2d_interpolated') represent different type of data provided by that component.
### The value lists define which canvases (rerun windows) will show the data
### 1. '3D_Canvas': This shows data in the 3D canvas. It is only for multi-view datasets. (Do not change the canvas name).
### 2. Cameras: Data will be visualized on that specific camera image. The camera name must match the camera placeholder names in dataset_properties.toml
### 3. Metrics - The displays the data as plots.
### 4. Empty list: If you don't want the data to be visualized, leave the list empty.

### Configuring Algorithm Display
## Under 'media.component', the algorithms parameter let you choose which algorithms to display.
## For example, if you have multiple algorithms (e.g., hrnetw48 and vitpose in the body_joints component),
## you can specify which algorithmâ€™s results to show.
## If you want to see results from both algorithms, list both names.

### Configuring Apearance
## Under 'media.component.appearance', you can configure the color and radii (the size of the dots and lines).

# Component: gaze individual
[media.gaze_individual]
algorithms = ['multiview_eth_xgaze']            # list of algorithms

[media.gaze_individual.canvas]
3d_filtered = ["3D_Canvas", "<cam_face1>", "<cam_face2>", "<cam_top>", "<cam_front>"] ## key options 3d, 3d_filtered ## value options: [3D_Canvas], [3D_Canvas, camera names], [camera names], []
## Note: Delete '3D_Canvas' if you don't have a multi-view setup.

[media.gaze_individual.appearance]
colors = [[0,150, 90]]                  # define the color of individual gaze
radii = {'3d'= 0.01, 'camera_view'= 4}  # define the size of gaze arrow in 3D_Canvas and camera views

# Component: gaze interaction
[media.gaze_interaction]
algorithms = ['gaze_distance']

[media.gaze_interaction.canvas]
gaze_look_at_3d = ["3D_Canvas", "<cam_face1>", "<cam_face2>"] ## value options: [3D_Canvas], [3D_Canvas, camera names], [camera names], []
#gaze_look_at_2d = ["<cam_front>"]                            ## use data key gaze_look_at_2d if you don't have a multi-view setup.

[media.gaze_interaction.appearance]
colors = [[[0,221, 22], [247, 13, 26]]] # first one is bright green and used when the person is
                                        # look other person's face. Second color (red) is use when the persopn does'n look other person's face
radii = {}


# Component: body joints
[media.body_joints]
algorithms = ['vitpose', 'hrnetw48']

[media.body_joints.canvas]
3d = ["3D_Canvas"] ## value options: 3D_Canvas, [] -- Note: set as [] if you don't have a multi-view setup.
2d_interpolated = ["<cam_top>", "<cam_front>"]  ##key options: 2d, 2d_interpolated, 2d_filtered ##value options: [camera names]

[media.body_joints.appearance]
colors = [[0, 191, 255], [200, 200, 200]]
radii = {'3d'= 0.01, 'camera_view'= 4}

# Component: hand joints
[media.hand_joints]
algorithms = ['hrnetw48']

[media.hand_joints.canvas]
3d = ["3D_Canvas"] ## value options: 3D_Canvas, [] -- Note: set as [] if you don't have a multi-view setup.
2d_interpolated = ["<cam_top>", "<cam_front>"]  ##key options: 2d, 2d_interpolated, 2d_filtered ##value options: [camera names]

[media.hand_joints.appearance]
colors = [[255, 69, 0]]  # orange #alternative 255, 165, 0
radii = {'3d'= 0.006, 'camera_view'= 2}


# Component: face landmarks
[media.face_landmarks]
algorithms = ['hrnetw48']

[media.face_landmarks.canvas]
3d = ["3D_Canvas"] ## value options: 3D_Canvas, [] -- Note: set as [] if you don't have a multi-view setup.
2d_interpolated = ["<cam_top>", "<cam_front>"]  ##key options: 2d, 2d_interpolated, 2d_filtered ##value options: [camera names]

[media.face_landmarks.appearance]
colors =  [[170,171,69]] #[255,255,4] yellow =- green j. [77, 188, 139]
radii = {'3d'= 0.01, 'camera_view'= 4}


# Component: emotion_individual
[media.emotion_individual]
algorithms = ['py_feat']

[media.emotion_individual.canvas]
emotions = ["<cam_top>", "<cam_front>", "<cam_face1>", "<cam_face2>"]  ##value options: [camera names]

[media.emotion_individual.appearance]
colors =  [[[255,0,0], [85,170,47], [72,61,139], [255,215,0], [70,130,180], [255,140,0], [128, 128, 128]]] #red, dark olive green, dark stale blue, gold, steel blue, dark orange, gray  
radii = {'3d'= 0.01, 'camera_view'= 4}

# Component: head orientation
[media.head_orientation]
algorithms = ['spiga']

[media.head_orientation.canvas]
head_orientation_2d = ["<cam_front>", "<cam_top>", "<cam_face1>", "<cam_face2>"]  ##value options: [camera names]

[media.head_orientation.appearance]
colors =  [[[255,92,0], [85,170,47]]] #neon orange, dark olive green
radii = {'3d'= 0.01, 'camera_view'= 4}

# Component: kinematics
[media.kinematics]
algorithms = ['velocity_body']

[media.kinematics.canvas]
velocity_body_3d = ["metric_velocity"] # if don't have multi-view, use velocity_body_2d
#velocity_body_2d = ["metric_velocity"]

[media.kinematics.joints] ## visualize the mean velocity for the given bodyparts.
"head" = ["nose","left_eye","right_eye","left_ear","right_ear"]
"upper_body" = ["left_shoulder","right_shoulder","left_elbow", "right_elbow", "left_wrist", "right_wrist"]  # Indices of keypoints belonging to the upper body
"lower_body" = ["left_hip", "right_hip", "left_knee", "right_knee", "left_ankle", "right_ankle"]


# Component: proximity
[media.proximity]
algorithms = ['body_distance']

[media.proximity.canvas]
body_distance_3d = [] ## value options: 3D_Canvas, [] -- Note: set as [] if you don't have a multi-view setup.
body_distance_2d = ["<cam_top>", "<cam_front>"] ## value options: [camera nanes]

[media.proximity.appearance]
colors = [[153,50,204]]
radii = {'3d'= 0.008, 'camera_view'= 3}
