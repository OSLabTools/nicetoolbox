# This config supports the use of placeholders.
# options are, e.g., all keys from your local 'machine_specific_paths.toml'

# gaze_individual
[algorithms.xgaze_3cams]
input_data_format = "frames"
camera_names = ["<cam_face1>", "<cam_face2>", "<cam_top>", "<cam_front>"]
env_name = "venv:xgaze_3cams"
shape_predictor_filename = "<assets>/xgaze_3cams/shape_predictor_68_face_landmarks.dat"
face_model_filename = "<assets>/xgaze_3cams/face_model.txt"
pretrained_model_filename = "<assets>/xgaze_3cams/epoch_24_ckpt.pth.tar"
face_detector_filename = "<assets>/xgaze_3cams/mmod_human_face_detector.dat"
log_frame_idx_interval = 10
filtered = true
window_length = 11
polyorder = 2

# body_joints, hand_joints, face_landmarks
[frameworks.mmpose]
input_data_format = "frames"
camera_names = ["<cam_top>", "<cam_front>"]
env_name = "conda:openmmlab"
multi_person = true
save_images = true
resolution = [1000, 700]
device = "cuda:0"
filtered = true
window_length = 11
polyorder = 2
3d_results = true

[algorithms.hrnetw48]
framework = "mmpose"
pose_config = "td-hm_hrnet-w48_8xb32-210e_coco-wholebody-384x288"
pose_checkpoint = "<assets>/mmpose/configs/hrnet_w48_coco_wholebody_384x288_dark-f5726563_20200918.pth"
detection_config = "<assets>/mmpose/configs/faster_rcnn_r50_fpn_coco.py"
detection_checkpoint = "<assets>/mmpose/configs/faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth"
keypoint_mapping = "coco_wholebody"
min_detection_confidence = 0.5

[algorithms.vitpose]
framework = "mmpose"
pose_config = "<assets>/mmpose/configs/td-hm_ViTPose-large_8xb64-210e_coco-256x192.py"
pose_checkpoint = "<assets>/mmpose/configs/td-hm_ViTPose-large_8xb64-210e_coco-256x192-53609f55_20230314.pth"
detection_config = "<assets>/mmpose/configs/faster_rcnn_r50_fpn_coco.py"
detection_checkpoint = "<assets>/mmpose/configs/faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth"
keypoint_mapping = "coco_wholebody"
min_detection_confidence = 0.5

# kinematics
[algorithms.velocity_body]
input_detector_names = [["body_joints", "hrnetw48"]]

# proximity
[algorithms.body_distance]
used_keypoints = ["nose"]  # if the list is more than one keypoint, the midpoint
input_detector_names = [["body_joints", "hrnetw48"]]

# leaning
[algorithms.body_angle]
used_keypoints = [["left_shoulder", "right_shoulder"], ["left_hip", "right_hip"], ["left_knee", "right_knee"]]
input_detector_names = [["body_joints", "hrnetw48"]]

# gaze_interaction
[algorithms.gaze_distance]
camera_names = ["3d"]
input_detector_names = [["gaze_individual", "xgaze_3cams"], ["face_landmarks", "hrnetw48"]]
keypoint_mapping = "coco_wholebody"
threshold_look_at = 0.4

# emotion_individual
[algorithms.py_feat]
input_data_format = "frames"
camera_names = ["<cam_face1>", "<cam_face2>", "<cam_top>", "<cam_front>"]
env_name = "venv:py_feat"
log_frame_idx_interval = 10
batch_size = 20  # should be less than number of frames, consider available RAM
max_cores = -1  # -1 will use all available cores