# This config supports the use of placeholders.
# options are, e.g., all keys from your local 'machine_specific_paths.toml'

# gaze_individual
[algorithms.multiview_eth_xgaze]
input_data_format = "frames"
camera_names = ["<cur_cam_face1>", "<cur_cam_face2>", "<cur_cam_top>", "<cur_cam_front>"]
env_name = "venv:multiview_eth_xgaze"
shape_predictor_filename = "<assets>/multiview_eth_xgaze/shape_predictor_68_face_landmarks.dat"
face_model_filename = "<assets>/multiview_eth_xgaze/face_model.txt"
pretrained_model_filename = "<assets>/multiview_eth_xgaze/epoch_24_ckpt.pth.tar"
face_detector_filename = "<assets>/multiview_eth_xgaze/mmod_human_face_detector.dat"
log_frame_idx_interval = 200
filtered = true
window_length = 11
polyorder = 2

# body_joints, hand_joints, face_landmarks
[frameworks.mmpose]
input_data_format = "frames"
camera_names = ["<cur_cam_top>", "<cur_cam_front>"]
env_name = "conda:openmmlab"
multi_person = true
save_images = true
resolution = [1000, 700]
device = "cuda:0"
filtered = true
window_length = 11
polyorder = 2
3d_results = true

[algorithms.hrnetw48]
framework = "mmpose"
pose_config = "td-hm_hrnet-w48_8xb32-210e_coco-wholebody-384x288"
pose_checkpoint = "<assets>/mmpose/configs/hrnet_w48_coco_wholebody_384x288_dark-f5726563_20200918.pth"
detection_config = "<assets>/mmpose/configs/faster_rcnn_r50_fpn_coco.py"
detection_checkpoint = "<assets>/mmpose/configs/faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth"
keypoint_mapping = "coco_wholebody"
min_detection_confidence = 0.5

[algorithms.vitpose]
framework = "mmpose"
pose_config = "td-hm_ViTPose-large_8xb64-210e_coco-256x192"
pose_checkpoint = "<assets>/mmpose/configs/td-hm_ViTPose-large_8xb64-210e_coco-256x192-53609f55_20230314.pth"
detection_config = "<assets>/mmpose/configs/faster_rcnn_r50_fpn_coco.py"
detection_checkpoint = "<assets>/mmpose/configs/faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth"
keypoint_mapping = "coco_wholebody"
min_detection_confidence = 0.5

# kinematics
[algorithms.velocity_body]
input_detector_names = [["body_joints", "hrnetw48"]]

# proximity
[algorithms.body_distance]
used_keypoints = ["nose"]  # if the list is more than one keypoint, the midpoint
input_detector_names = [["body_joints", "hrnetw48"]]

# leaning
[algorithms.body_angle]
used_keypoints = [["left_shoulder", "right_shoulder"], ["left_hip", "right_hip"], ["left_knee", "right_knee"]]
input_detector_names = [["body_joints", "hrnetw48"]]

# gaze_interaction
[algorithms.gaze_distance]
# camera_names = ["3d"]
input_detector_names = [["gaze_individual", "multiview_eth_xgaze"], ["face_landmarks", "hrnetw48"]]
keypoint_mapping = "coco_wholebody"
threshold_look_at = 0.4

[algorithms.gaze_fusion]
input_detector_names = [["gaze_individual", "multiview_eth_xgaze"]]
fusion_method = "weighted_average"  # options: "average", "weighted_average"
filtered = true
window_length = 11
polyorder = 2
ensemble_enabled = true
# ensemble_weights = [0.5, 0.5]  # TODO should we add this?

# emotion_individual
[algorithms.py_feat]
input_data_format = "frames"
camera_names = ["<cur_cam_face1>", "<cur_cam_face2>", "<cur_cam_top>", "<cur_cam_front>"]
env_name = "venv:py_feat"
log_frame_idx_interval = 200
# For batch_size, ensure:
# [(batch_size) * (~Size of one frame (GB))] < Available vRAM (RAM) (GB)
batch_size = 50
max_cores = -1  # -1 will use all available cores

# head_orientation
[algorithms.spiga]
input_data_format = "frames"
camera_names = ["<cur_cam_face1>", "<cur_cam_face2>", "<cur_cam_top>", "<cur_cam_front>"]
env_name = "venv:spiga"
log_frame_idx_interval = 200
batch_size = 32
# Choose the SPIGA model configuration:
# "wflw" — Wider Facial Landmarks in-the-Wild (98 landmarks)
# "300w" — 300-W dataset (68 landmarks)
# "ibug" — iBUG challenging subset (68 landmarks, variation-heavy)
model_config = "wflw"