# This config supports using placeholders.
# options are <git_hash> <commit_message> <me> <today> <yyyymmdd> <time> <pwd>
# and all keys from your local 'machine_specific_paths.toml'

git_hash = "<git_hash>"
# the inout video is processed into 'frames', 'segments' and 'snippets'
video_length = 33300  # in frames, relevant for all ##900
video_start = 0   # in frames, relevant for all  ##14400
video_skip_frames = false  # relevant for frames only
annotation_interval = 2  # in seconds, length of segments

[io]
dataset_name = "ours"
process_data_to = "data_folder"  # options are 'data_folder' and 'tmp_folder'
data_folder = "<pis_folder_path>/raw_processed/isa_tool_input/<dataset_name>_<participant_ID>"
tmp_folder = "<pis_folder_path>/experiments/tmp"
out_folder = "<pis_folder_path>/experiments/<yyyymmdd>"
method_out_folder = "<out_folder>/<method_name>/method_output"
method_visualization_folder = "<out_folder>/<method_name>/visualization"
method_additional_output_folder = "<out_folder>/<method_name>/additional_output"
method_tmp_folder = "<tmp_folder>/<method_name>"
method_final_result_folder = "<out_folder>/<method_name>"
code_folder = "<pwd>"

[datasets.ours]
participant_ID = "PIS_ID_000"
video_folder = "<pis_folder_path>/<participant_ID>/IOI/compressed_25/withAudio"
calibration_file = "<pis_folder_path>/<participant_ID>/Calibration/rescaled25/camera_params_calibio_2020-08-11.json"
cam_front = 'cam4'
cam_top = 'cam3'
cam_face1 = 'cam1'
cam_face2 = 'cam2'
subjects_descr = ["personL", "personR"]  # implies there are 2 people visible in the video

[datasets.mpi_inf_3dhp]
participant_ID = "S1"
video_folder = "<datasets_folder_path>/mpi_inf_3dhp/<participant_ID>/Seq1/imageSequence"
calibration_file = "<datasets_folder_path>/mpi_inf_3dhp/<participant_ID>/Seq1/camera.calibration"
cam_front = 'video_8'
cam_top = 'video_5'
cam_face1 = ''
cam_face2 = ''
subjects_descr = ["person"]  # implies that the video shows a single person

[methods]
names = ["mmpose"]

[methods.nodding_pigeon]
input_data_format = "segments"
camera_names = ["<cam_face1>", "<cam_face2>"]
env_name = "venv:env"

[methods.ethXgaze]
input_data_format = "frames"
camera_names = ["<cam_face1>", "<cam_face2>"]
shape_predictor_filename = "<methods_folder_path>/ethXgaze/ethXgaze/modules/shape_predictor_68_face_landmarks.dat"
face_model_filename = "<methods_folder_path>/ethXgaze/ethXgaze/face_model.txt"
pretrained_model_filename = "<methods_folder_path>/ethXgaze/ethXgaze/ckpt/epoch_24_ckpt.pth.tar"

[methods.xgaze_3cams]
input_data_format = "frames"
camera_names = ["<cam_face1>", "<cam_face2>", "<cam_top>", "<cam_front>"]
env_name = "venv:env"
shape_predictor_filename = "<pwd>/third_party/xgaze_3cams/xgaze_3cams/shape_predictor_68_face_landmarks.dat"
face_model_filename = "<pwd>/third_party/xgaze_3cams/xgaze_3cams/face_model.txt"
pretrained_model_filename = "<methods_folder_path>/ethXgaze/ethXgaze/ckpt/epoch_24_ckpt.pth.tar"
face_detector_filename = "<pwd>/third_party/xgaze_3cams/xgaze_3cams/mmod_human_face_detector.dat"

[methods.mmpose]
input_data_format = "frames"
camera_names = ["<cam_top>", "<cam_front>"]
algorithm = "vitpose"
env_name = "conda:openmmlab"
multi_person = true
save_images = true
resolution = [1000, 700] # ToDo - after implement bbox intersection calculation, get rid of this
device = "cuda:0"
filtered = true #use falsed if you run for evaluation
window_length = 11
polyorder = 2

[methods.mmpose.vitpose]
pose_config = "./third_party/mmpose/configs/td-hm_ViTPose-large_8xb64-210e_coco-256x192.py"
pose_checkpoint = "https://download.openmmlab.com/mmpose/v1/body_2d_keypoint/topdown_heatmap/coco/td-hm_ViTPose-large_8xb64-210e_coco-256x192-53609f55_20230314.pth"
detection_config = "./third_party/mmpose/configs/faster_rcnn_r50_fpn_coco.py"
detection_checkpoint = "./third_party/mmpose/configs/faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth"
keypoint_mapping = "coco_wholebody"
min_detection_confidence = 0.5

[methods.emoca]
input_data_format = "frames"
camera_names = ["<cam_face1>", "<cam_face2>"]
env_name = "venv:env"

[methods.active_speaker]
input_data_format = "snippets"
camera_names = ["<cam_top>", "<cam_front>"]
env_name = "venv:env"
ASC_model_weights = './third_party/active_speaker/checkpoints/SPELL_audiovisual_feature_extraction_checkpoints/resnet18-tsm-aug.pth'
ASC_model_name = 'resnet18-tsm-aug'
# Two options for the edge connection mode.
# 'csi': Connect the nodes only with the same identities across the frames.
# 'cdi': Connect different identities across the frames.
graph_ec_mode = 'csi'
graph_time_span = 90
graph_tau = 0.9
evaluation_type = 'AVA_ASD'
GraViT_model = './third_party/active_speaker/checkpoints/SPELL_ASD_default/ckpt_best.pt'
GraViT_config = './third_party/active_speaker/checkpoints/SPELL_ASD_default/cfg.yaml'

[features]
names = ["kinematics", "proximity", "leaning"]

[features.kinematics]
input_detector_names = ["mmpose_vitpose"]

[features.proximity]
used_keypoints = ["nose"] # if the list is more than one keypoint, the midpoint
input_detector_names = ["mmpose_vitpose"]

[features.leaning]
used_keypoints = [["left_shoulder", "right_shoulder"], ["left_hip", "right_hip"], ["left_knee", "right_knee"]]
input_detector_names = ["mmpose_vitpose"]

[features.gazeDistance]
input_detector_names = ["xgaze_3cams", "mmpose_vitpose"]
keypoint_mapping = "coco_wholebody"
